{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7zQveWs6Cxx"
   },
   "source": [
    "# federated-ml-health \n",
    "**Notatnik przystosowany do zajęć z SI w informatyce biomedycznej**.\n",
    "\n",
    "Copyright 2020 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tF3IEWxX2CKP"
   },
   "outputs": [],
   "source": [
    "# To install TFF and dependencies\n",
    "!pip install --quiet --upgrade tensorflow-federated\n",
    "!pip install --quiet --upgrade nest-asyncio\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KMB_hsVf4w0D"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "# Przygotowanie danych\n",
    "\n",
    "Na początku wykorzystamy zbiór `pima`. W dalszej kolejności będziemy pracować na odpowiednio przygotowanej wersji zbioru MIMIC-III."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "from collections import defaultdict \n",
    "from matplotlib.pyplot import figure\n",
    "from numpy import loadtxt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "leOpdF3N1P6F"
   },
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "tff.framework.set_default_context(tff.backends.native.create_thread_debugging_execution_context(clients_per_thread=50))\n",
    "np.random.seed(10)\n",
    "tf.random.set_seed(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ds = pd.read_csv('mimic3.csv')\n",
    "num_col = raw_ds.shape[1]\n",
    "X = raw_ds.iloc[:, 0:num_col-1].values\n",
    "y = raw_ds.iloc[:, num_col-1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Podział na część uczącą i testującą\n",
    "\n",
    "Tym razem porządniej -- `scaler` oraz `imputer` są uczone na danych uczących i stosowane do danych testowych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PROPORTION = 0.8\n",
    "NUM_FEATURES = X.shape[1]\n",
    "NUM_ROUNDS = 8\n",
    "n_train = round(TRAIN_PROPORTION * np.size(X, 0))\n",
    "\n",
    "X_train = X[:n_train]\n",
    "y_train =  y[:n_train]\n",
    "X_test = X[n_train:]\n",
    "y_test =  y[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline \n",
    "\n",
    "preprocessor = make_pipeline(SimpleImputer(), StandardScaler())\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "# D = np.column_stack((X, y))\n",
    "# ds = pd.DataFrame(\n",
    "#     data=D,\n",
    "#     columns=raw_ds.columns)\n",
    "# ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gmfdu_98F5rb"
   },
   "source": [
    "# Podejście scentralizowane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mR9evztMh702"
   },
   "source": [
    "## Regresja - scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "frzfmadVh4zx"
   },
   "outputs": [],
   "source": [
    "sk_model = LogisticRegression(random_state=0, solver='liblinear').fit(X_train, y_train)\n",
    "proba_test = sk_model.predict_proba(X_test)[:,1]\n",
    "fpr_skl_liblinear, tpr_skl_liblinear, threshold_skl_liblinear = sklearn.metrics.roc_curve(y_test, proba_test)\n",
    "roc_auc_skl_liblinear = sklearn.metrics.auc(fpr_skl_liblinear, tpr_skl_liblinear)\n",
    "print(f'AUC-LIN = {roc_auc_skl_liblinear:.4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_model = LogisticRegression(random_state=0, solver='sag', max_iter=500).fit(X_train, y_train)\n",
    "proba_test = sk_model.predict_proba(X_test)[:,1]\n",
    "fpr_skl_sag, tpr_skl_sag, threshold_skl_sag = sklearn.metrics.roc_curve(y_test, proba_test)\n",
    "roc_auc_skl_sag = sklearn.metrics.auc(fpr_skl_sag, tpr_skl_sag)\n",
    "print(f'AUC-SAG = {roc_auc_skl_sag:.4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_model = LogisticRegression(random_state=0, solver='lbfgs').fit(X_train, y_train)\n",
    "proba_test = sk_model.predict_proba(X_test)[:,1]\n",
    "fpr_skl, tpr_skl, threshold_skl = sklearn.metrics.roc_curve(y_test, proba_test)\n",
    "roc_auc_skl = sklearn.metrics.auc(fpr_skl, tpr_skl)\n",
    "print(f'AUC-SKL = {roc_auc_skl:.4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zv1I3oDbiBlx"
   },
   "source": [
    "## Regresja - TF\n",
    "\n",
    "Wyjaśnienie odnośnie definicji modelu (oryginalny projekt): *Adam optimization method is used to mimic the sklearn solver as close as possible (leveraging second derivatives of gradient).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r3VJqYAuiD7t"
   },
   "outputs": [],
   "source": [
    "dataset_train = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(n_train)\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(n_train)\n",
    "\n",
    "def create_keras_model():\n",
    "  return tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Dense(\n",
    "          1,\n",
    "          activation='sigmoid',\n",
    "          input_shape=(NUM_FEATURES,),\n",
    "          kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "      )\n",
    "  ])\n",
    "\n",
    "def create_keras_model_deeper():\n",
    "  initializer = tf.keras.initializers.GlorotNormal(seed=10)\n",
    "  m = tf.keras.models.Sequential()\n",
    "  m.add(tf.keras.Input(shape=(NUM_FEATURES,)))\n",
    "  m.add(tf.keras.layers.Dense(6, activation='sigmoid', kernel_initializer=initializer))\n",
    "  m.add(tf.keras.layers.Dense(3, activation='sigmoid', kernel_initializer=initializer))\n",
    "  m.add(tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=initializer, kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.0001, l2=0.01)))\n",
    "  return m\n",
    "  \n",
    "\n",
    "tf_model = create_keras_model()\n",
    "tf_model.compile(\n",
    "              optimizer=tf.keras.optimizers.Nadam(learning_rate=0.5),   \n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[\n",
    "                       tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "                       tf.keras.metrics.AUC(name='auc'),\n",
    "                       ]\n",
    "              )\n",
    "\n",
    "batch_size = round(n_train/3)\n",
    "tf_model.fit(dataset_train, validation_data=dataset_test, epochs=NUM_ROUNDS, batch_size=batch_size, verbose=1, use_multiprocessing=True)\n",
    "\n",
    "proba_test = tf_model.predict(dataset_test)\n",
    "fpr_tf, tpr_tf, threshold = sklearn.metrics.roc_curve(y_test, proba_test)\n",
    "roc_auc_tf = sklearn.metrics.auc(fpr_tf, tpr_tf)\n",
    "print(f'AUC-TF = {roc_auc_tf:0.4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7hRe3fwiM54"
   },
   "source": [
    "# Regresja - TF Federated\n",
    "\n",
    "Utworzenie zbioru z danymi uczącymi, aby ułatwić przydział danych do poszczególnych klientów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_train = pd.DataFrame(data=X_train, columns=raw_ds.columns[:-1])\n",
    "df_labels_train = pd.DataFrame(data=y_train, columns=raw_ds.columns[-1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przypisanie identyfikatorów (indeksów) przykładów uczących do poszczególnych klientów. Obecnie wszyscy klienci otrzymują taką samą liczbę przykładów, przy czym rozkład klas nie jest zachowywany."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_samples_to_clients(data, n_clients):\n",
    "    from sklearn.model_selection import KFold\n",
    "    client_sample_ids = []\n",
    "    kf = KFold(n_splits=n_clients, shuffle=True, random_state=42)\n",
    "    for _, test_ids in kf.split(data):\n",
    "        client_sample_ids.append(test_ids)\n",
    "    return client_sample_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLIENTS = 20\n",
    "NUM_PARTICIPATING_PER_ROUND = round(NUM_CLIENTS/3)\n",
    "client_ids = list(range(NUM_CLIENTS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_sample_ids = assign_samples_to_clients(df_data_train, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_client_dataset(data, labels, client_ids, client_sample_ids):\n",
    "  def create_dataset_fn(client_id):\n",
    "    sample_ids = client_sample_ids[client_id]\n",
    "    return tf.data.Dataset.from_tensor_slices((data.iloc[sample_ids, :].values, labels.iloc[sample_ids, :].values))\n",
    "\n",
    "  return tff.simulation.datasets.ClientData.from_clients_and_tf_fn(\n",
    "      client_ids=client_ids,\n",
    "      serializable_dataset_fn=create_dataset_fn)\n",
    "  \n",
    "def preprocess(dataset):\n",
    "    card = dataset.cardinality()\n",
    "    batch_size = 1 if card ==  tf.data.INFINITE_CARDINALITY or tf.data.UNKNOWN_CARDINALITY else round(card.numpy()/3)\n",
    "    return dataset.batch(batch_size)\n",
    "\n",
    "def make_federated_data(client_data, client_ids):\n",
    "  return [\n",
    "      preprocess(client_data.create_tf_dataset_for_client(id))\n",
    "      for id in client_ids\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_dataset_train = create_client_dataset(df_data_train, df_labels_train, client_ids, client_sample_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_example_dataset = preprocess(client_dataset_train.create_tf_dataset_for_client(client_ids[0]))\n",
    "\n",
    "def model_fn():\n",
    "  keras_model = create_keras_model()\n",
    "  return tff.learning.from_keras_model(\n",
    "      keras_model,\n",
    "      input_spec=preprocessed_example_dataset.element_spec,\n",
    "      loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "      metrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "               tf.keras.metrics.AUC(name='auc')])\n",
    "  \n",
    "# Create TFF interative process.\n",
    "iterative_process = tff.learning.build_federated_averaging_process(\n",
    "    model_fn,\n",
    "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0),\n",
    "    server_optimizer_fn=lambda: tf.keras.optimizers.Nadam(learning_rate=0.5),\n",
    "    use_experimental_simulation_loop = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = iterative_process.initialize()\n",
    "tff_model = create_keras_model()\n",
    "tff_auc = defaultdict(lambda:0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xjbbq1mkiUf2"
   },
   "outputs": [],
   "source": [
    "# Test various sizes of subsets of eligible devices participating in each round.\n",
    "for n_clients in list(range(1, NUM_CLIENTS, 5)):\n",
    "  for i_round in range(NUM_ROUNDS):\n",
    "    federated_train_data = make_federated_data(client_dataset_train, np.random.choice(range(NUM_CLIENTS), size=n_clients, replace=False))\n",
    "    state, metrics = iterative_process.next(state, federated_train_data)\n",
    "    print(n_clients, i_round, str(metrics))\n",
    "    state.model.assign_weights_to(tff_model)\n",
    "    proba_test = tff_model.predict(dataset_test)\n",
    "    fpr, tpr, _ = sklearn.metrics.roc_curve(y_test, proba_test)\n",
    "    test_auc = sklearn.metrics.auc(fpr, tpr)\n",
    "    test_loss = tf.keras.losses.binary_crossentropy(y_test, np.reshape(proba_test, [-1]))\n",
    "    print(f'AUC = {test_auc:0.4}, Loss={test_loss:0.4}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EFON97SnQVK5"
   },
   "outputs": [],
   "source": [
    "state.model.assign_weights_to(tff_model)\n",
    "proba_test = tff_model.predict(dataset_test)\n",
    "fpr_tff_sgd, tpr_tff_sgd, threshold_tff_sgd = sklearn.metrics.roc_curve(y_test, proba_test)\n",
    "roc_auc_tff_sgd = sklearn.metrics.auc(fpr_tff_sgd, tpr_tff_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'AUC-TFF = {roc_auc_tff_sgd:0.4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porównanie stworzonych modeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MHWXzjiRic62"
   },
   "outputs": [],
   "source": [
    "figure(num=None, figsize=(8, 6), dpi=150, facecolor='w', edgecolor='k')\n",
    "plt.title('ROC')\n",
    "plt.plot(fpr_skl_liblinear, tpr_skl_liblinear, label = 'sklearn LR LIN AUC = %0.3f' % roc_auc_skl_liblinear)\n",
    "plt.plot(fpr_skl_sag, tpr_skl_sag, label = 'sklearn LR SAG AUC = %0.3f' % roc_auc_skl_sag)\n",
    "plt.plot(fpr_tf, tpr_tf, label = 'TF Centralized LR AUC = %0.3f' % roc_auc_tf)\n",
    "plt.plot(fpr_tff_sgd, tpr_tff_sgd, label = 'TF Federated LR SGDM AUC = %0.3f' % roc_auc_tff_sgd)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "c7zQveWs6Cxx"
   ],
   "name": "federated-ml-health",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
