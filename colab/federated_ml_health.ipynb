{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7zQveWs6Cxx"
      },
      "source": [
        "# federated-ml-health \n",
        "**Notatnik przystosowany do zajęć z SIwIB**.\n",
        "* aktualizacja do nowszej wersji biblioteki TF Federated\n",
        "* uproszczenie kodu (usunięcie części związanej z *Differential Privacy*) i przygotowanie fragmentów kodu na potrzeby zajęć\n",
        "\n",
        "---\n",
        "\n",
        "Oryginalna wersja: https://github.com/google/federated-ml-health\n",
        "\n",
        "Copyright 2020 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tF3IEWxX2CKP"
      },
      "outputs": [],
      "source": [
        "# Instalacja TF Federated\n",
        "%pip install --quiet --upgrade tensorflow-federated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMB_hsVf4w0D"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# Przygotowanie danych\n",
        "\n",
        "Na początku wykorzystamy zbiór `pima`. W dalszej kolejności będziemy pracować na odpowiednio przygotowanej wersji zbioru MIMIC-III (dostępny na eKursach)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdFlgnj-Cu1N"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "import nest_asyncio\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "from collections import defaultdict \n",
        "from matplotlib.pyplot import figure\n",
        "from numpy import loadtxt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0V5rprpCu1N"
      },
      "outputs": [],
      "source": [
        "# Ukrycie części niestotnych logów\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Podmontowanie dysku Google\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-03PDFIvEkNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leOpdF3N1P6F"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFrMbx2HCu1N"
      },
      "outputs": [],
      "source": [
        "# Ścieżki do plików przy założeniu, że pliki z danymi znajdują się w głównym katalogu na GDrive\n",
        "csv_fn = '/content/drive/MyDrive/pima.csv' \n",
        "# W dalszej kolejności będziemy używać '/content/drive/MyDrive/mimic3.csv'\n",
        "raw_ds = pd.read_csv(csv_fn)\n",
        "num_col = raw_ds.shape[1]\n",
        "# Zakładamy, że atrybut decyzyjny jest zawsze w ostatniej kolumnie\n",
        "X = raw_ds.iloc[:, 0:num_col-1].values\n",
        "y = raw_ds.iloc[:, num_col-1].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_e9bgXfCu1N"
      },
      "source": [
        "## Podział na część uczącą i testującą\n",
        "\n",
        "Tym razem porządniej, niż w oryginalnym notatniku :) -- `scaler` oraz `imputer` są uczone na danych uczących i stosowane do danych testowych."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNFL56ALCu1N"
      },
      "outputs": [],
      "source": [
        "TRAIN_PROPORTION = 0.8\n",
        "NUM_FEATURES = X.shape[1]\n",
        "NUM_ROUNDS = 12\n",
        "\n",
        "n_train = round(TRAIN_PROPORTION * X.shape[0])\n",
        "n_test = X.shape[0] - n_train\n",
        "\n",
        "X_train = X[:n_train]\n",
        "y_train =  y[:n_train]\n",
        "X_test = X[n_train:]\n",
        "y_test =  y[n_train:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5CRuYtLCu1O"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import make_pipeline \n",
        "\n",
        "preprocessor = make_pipeline(SimpleImputer(), StandardScaler())\n",
        "\n",
        "X_train = preprocessor.fit_transform(X_train)\n",
        "X_test = preprocessor.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gmfdu_98F5rb"
      },
      "source": [
        "# Podejście scentralizowane"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR9evztMh702"
      },
      "source": [
        "## Regresja - scikit-learn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testujemy kilka wariantów regresji logistycznej, aby uzyskać *baseline*."
      ],
      "metadata": {
        "id": "VhA4-aVkGfhT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frzfmadVh4zx"
      },
      "outputs": [],
      "source": [
        "sk_model = LogisticRegression(random_state=42).fit(X_train, y_train)\n",
        "proba_test = sk_model.predict_proba(X_test)[:,1]\n",
        "fpr_sk, tpr_sk, threshold_sk = sklearn.metrics.roc_curve(y_test, proba_test)\n",
        "auc_sk = sklearn.metrics.auc(fpr_sk, tpr_sk)\n",
        "print(f'AUC-LIN = {roc_auc_sk:.4}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zv1I3oDbiBlx"
      },
      "source": [
        "## Regresja - TF\n",
        "\n",
        "Implementujemy regresję logistyczną w TF. Stworzony model (`tf_model`) wykorzystuje `Adam`-a. Wyjaśnienie autorów notatnika: *Adam optimization method is used to mimic the sklearn solver as close as possible (leveraging second derivatives of gradient).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zn4P2LPWCu1O"
      },
      "outputs": [],
      "source": [
        "# dataset_train = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(n_train)\n",
        "# dataset_test = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(n_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3VJqYAuiD7t"
      },
      "outputs": [],
      "source": [
        "# Prosty model - LR\n",
        "def create_keras_model():\n",
        "  return tf.keras.models.Sequential([\n",
        "      tf.keras.layers.Dense(\n",
        "          1,\n",
        "          activation='sigmoid',\n",
        "          input_shape=(NUM_FEATURES,),\n",
        "          kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
        "      )\n",
        "  ])\n",
        "\n",
        "# Bardziej złożony model - MLP\n",
        "def create_keras_model_deeper():\n",
        "  initializer = tf.keras.initializers.GlorotNormal(seed=10)\n",
        "  m = tf.keras.models.Sequential()\n",
        "  m.add(tf.keras.Input(shape=(NUM_FEATURES,)))\n",
        "  m.add(tf.keras.layers.Dense(6, activation='sigmoid', kernel_initializer=initializer))\n",
        "  m.add(tf.keras.layers.Dense(3, activation='sigmoid', kernel_initializer=initializer))\n",
        "  m.add(tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=initializer, kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.0001, l2=0.01)))\n",
        "  return m\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyAes_znCu1O"
      },
      "outputs": [],
      "source": [
        "tf_model = create_keras_model()\n",
        "tf_model.compile(\n",
        "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.5),   \n",
        "  loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "  metrics=[\n",
        "    tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "    tf.keras.metrics.AUC(name='auc'),\n",
        "  ]\n",
        ")\n",
        "\n",
        "batch_size = round(n_train/3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_L7jik4ECu1O"
      },
      "outputs": [],
      "source": [
        "# tf_model.fit(dataset_train, validation_data=dataset_test, epochs=NUM_ROUNDS, batch_size=batch_size, verbose=1, use_multiprocessing=True)\n",
        "\n",
        "tf_model.fit(X_train, y_train, epochs=NUM_ROUNDS, batch_size=batch_size, verbose=1, use_multiprocessing=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "proba_test = tf_model.predict(X_test)\n",
        "fpr_tf, tpr_tf, threshold = sklearn.metrics.roc_curve(y_test, proba_test)\n",
        "auc_tf = sklearn.metrics.auc(fpr_tf, tpr_tf)\n",
        "print(f'AUC-TF = {auc_tf:0.4}')"
      ],
      "metadata": {
        "id": "G5KI4yi1JqXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7hRe3fwiM54"
      },
      "source": [
        "# Regresja - TF Federated\n",
        "\n",
        "Utworzenie zbioru z danymi uczącymi, aby ułatwić przydział danych do poszczególnych klientów"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RhMqRjgCu1P"
      },
      "outputs": [],
      "source": [
        "df_X_train = pd.DataFrame(data=X_train, columns=raw_ds.columns[:-1])\n",
        "df_y_train = pd.DataFrame(data=y_train, columns=raw_ds.columns[-1:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JdxUFL3Cu1P"
      },
      "source": [
        "Przypisanie identyfikatorów (indeksów) przykładów uczących do poszczególnych klientów. Obecnie wszyscy klienci otrzymują taką samą liczbę przykładów, przy czym rozkład klas nie jest zachowywany. Ta funkcja powinna zostać zmodyfikowna w ramach projektu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkXA0vs7Cu1P"
      },
      "outputs": [],
      "source": [
        "def assign_samples_to_clients(data, n_clients):\n",
        "    from sklearn.model_selection import KFold\n",
        "    client_sample_ids = []\n",
        "    kf = KFold(n_splits=n_clients, shuffle=True, random_state=42)\n",
        "    for _, test_ids in kf.split(data):\n",
        "        client_sample_ids.append(test_ids)\n",
        "    return client_sample_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1E34xyICu1P"
      },
      "outputs": [],
      "source": [
        "NUM_CLIENTS = 20\n",
        "# NUM_PARTICIPATING_PER_ROUND = round(NUM_CLIENTS/3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buRxPV9TCu1P"
      },
      "outputs": [],
      "source": [
        "client_ids = list(range(NUM_CLIENTS))\n",
        "client_sample_ids = assign_samples_to_clients(X_train, NUM_CLIENTS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqPkkA22Cu1P"
      },
      "outputs": [],
      "source": [
        "def create_client_dataset(data, labels, client_ids, client_sample_ids):\n",
        "  def create_dataset_fn(client_id):\n",
        "    sample_ids = client_sample_ids[client_id]\n",
        "    return tf.data.Dataset.from_tensor_slices((data[sample_ids, :], labels[sample_ids]))\n",
        "\n",
        "  return tff.simulation.datasets.ClientData.from_clients_and_tf_fn(\n",
        "      client_ids=client_ids,\n",
        "      serializable_dataset_fn=create_dataset_fn)\n",
        "  \n",
        "def preprocess(dataset):\n",
        "    card = dataset.cardinality()\n",
        "    batch_size = 1 if card == tf.data.INFINITE_CARDINALITY or tf.data.UNKNOWN_CARDINALITY else round(card.numpy()/3)\n",
        "    return dataset.batch(batch_size)\n",
        "\n",
        "def make_federated_data(client_data, client_ids):\n",
        "  return [\n",
        "      preprocess(client_data.create_tf_dataset_for_client(id))\n",
        "      for id in client_ids\n",
        "  ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yob66S7XCu1P"
      },
      "outputs": [],
      "source": [
        "client_dataset_train = create_client_dataset(X_train, y_train, client_ids, client_sample_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdKLOqLyCu1P"
      },
      "outputs": [],
      "source": [
        "spec_dataset = preprocess(client_dataset_train.create_tf_dataset_for_client(client_ids[0]))\n",
        "\n",
        "def model_fn():\n",
        "  keras_model = create_keras_model()\n",
        "  return tff.learning.models.from_keras_model(\n",
        "    keras_model,\n",
        "    input_spec=spec_dataset.element_spec,\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    metrics=[\n",
        "        tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "        tf.keras.metrics.AUC(name='auc')\n",
        "    ]\n",
        "  )\n",
        "  \n",
        "# Tworzymy iteracyjny proces uczący z wykorzystaniem bazowego algorytmu FedAvg\n",
        "trainer = tff.learning.algorithms.build_weighted_fed_avg(\n",
        "    model_fn,\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.5), \n",
        "    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0),\n",
        "    use_experimental_simulation_loop = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIHapRY9Cu1P"
      },
      "outputs": [],
      "source": [
        "tff_auc = defaultdict(lambda:0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjbbq1mkiUf2"
      },
      "outputs": [],
      "source": [
        "# Zewnętrzna pętla pozwala na sprawdzanie różnej liczby klientów biorących udział w każdej rundzie obliczeń.\n",
        "# Na potrzeby projektu należy założyć, że wszyscy klienci uczestniczą w obliczeniach\n",
        "\n",
        "possible_num_clients_per_round = list(range(2, NUM_CLIENTS, 4))\n",
        "\n",
        "if not NUM_CLIENTS in possible_num_clients_per_round:\n",
        "  possible_num_clients_per_round.append(NUM_CLIENTS)\n",
        "\n",
        "for num_clients_per_round in possible_num_clients_per_round:\n",
        "  print(f\"# participating clients = {num_clients_per_round}\")\n",
        "  \n",
        "  state = trainer.initialize()\n",
        "  tff_model = create_keras_model()\n",
        "\n",
        "  for r in range(NUM_ROUNDS):\n",
        "    participating_client_ids = np.random.choice(range(NUM_CLIENTS), size=num_clients_per_round, replace=False)\n",
        "    print(f\"round {r + 1}/{NUM_ROUNDS} | participants = {participating_client_ids}\")\n",
        "    federated_train_data = make_federated_data(client_dataset_train, participating_client_ids)\n",
        "    state, metrics = trainer.next(state, federated_train_data)\n",
        "    # print(n_clients, i_round, str(metrics))\n",
        "\n",
        "  weights = trainer.get_model_weights(state)\n",
        "  weights.assign_weights_to(tff_model)\n",
        "\n",
        "  proba_test = tff_model.predict(X_test)\n",
        "  fpr_test, tpr_test, _ = sklearn.metrics.roc_curve(y_test, proba_test)\n",
        "  auc_test = sklearn.metrics.auc(fpr_test, tpr_test)\n",
        "  loss_test = tf.keras.losses.binary_crossentropy(y_test, np.reshape(proba_test, [-1]))\n",
        "  print(f'AUC = {auc_test:0.4}, Loss={loss_test:0.4}')\n",
        "\n",
        "  tff_auc[num_clients_per_round] = (auc_test, fpr_test, tpr_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMeDOEEbCu1P"
      },
      "source": [
        "### Porównanie stworzonych modeli"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHWXzjiRic62"
      },
      "outputs": [],
      "source": [
        "figure(num=None, figsize=(8, 6), dpi=150, facecolor='w', edgecolor='k')\n",
        "plt.title('ROC')\n",
        "plt.plot(fpr_sk, tpr_sk, label = f'sk-LR AUC = {auc_sk:0.3f}')\n",
        "plt.plot(fpr_tf, tpr_tf, label = f'tf-LR (centralized) AUC = {auc_tf:0.3f}')\n",
        "# Wyniki dla podejścia sfederowanego\n",
        "for num_participants, (auc_tff, fpr_tff, tpr_tff) in tff_auc.items():\n",
        "  plt.plot(fpr_tff, tpr_tff, label = f'tff-LR (federated, p = {num_participants}) AUC = {auc_tff:0.3f}')\n",
        "plt.legend(loc = 'lower right')\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1])\n",
        "plt.ylabel('TPR')\n",
        "plt.xlabel('FPR')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGBxq5VuCu1Q"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}